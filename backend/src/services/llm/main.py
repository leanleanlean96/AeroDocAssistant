import os 
import json
from uuid import uuid4
from typing import Any, Optional

from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct, VectorParams, Distance
from langchain_text_splitters import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv

from .prompts import chunking_prompt, main_prompt
from .knowledge_graph_queries import KnowledgeGraphQueries

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ Yandex GPT (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
try:
    from yandex_gpt import YandexGPT, YandexGPTConfigManager
except ImportError:
    try:
        from yandexgpt_python import YandexGPT, YandexGPTConfigManager
    except ImportError:
        YandexGPT = None
        YandexGPTConfigManager = None
        print("‚ö†Ô∏è  YandexGPT –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. LLM —Ñ—É–Ω–∫—Ü–∏–∏ –±—É–¥—É—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã.")


load_dotenv("./env/llm.env")
YANDEX_API_KEY = os.getenv("YANDEX_API_KEY")
CATALOG_ID = os.getenv("CATALOG_ID")


class RAGModel:
    def __init__(self, enable_knowledge_graph: bool = True) -> None:
        self.qdrant = QdrantClient(":memory:", prefer_grpc=False)
        self.embedding_model = SentenceTransformer("ai-forever/ru-en-RoSBERTa")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –º–æ–¥–µ–ª–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        self.llm_model = None
        if YandexGPT:
            try:
                self.llm_model = YandexGPT(
                    api_key=os.getenv("YANDEX_API_KEY", "dummy_key"),
                    catalog_id=os.getenv("CATALOG_ID", "dummy_catalog")
                )
            except Exception as e:
                print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å YandexGPT: {e}")
        
        # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≥—Ä–∞—Ñ–æ–º –∑–Ω–∞–Ω–∏–π
        self.knowledge_graph = None
        self.enable_kg = enable_knowledge_graph
        if enable_knowledge_graph:
            try:
                self.knowledge_graph = KnowledgeGraphQueries()
                print("‚úì –ì—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π —É—Å–ø–µ—à–Ω–æ –ø–æ–¥–∫–ª—é—á–µ–Ω")
            except Exception as e:
                print(f"‚ö† –ì—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                self.enable_kg = False


    def get_embeddings(self, text:str, task:str="–ø–æ–∏—Å–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º") -> list[float]:
        prefixed_text: str = f"{task}: {text}"
        embedding: list[float] = self.embedding_model.encode(
            prefixed_text,
            normalize_embeddings=True,
            convert_to_numpy=False,
            show_progress_bar=False
        ).tolist()
        return embedding


    def create_collection(self, collection_name: str) -> None:
        size = len(self.get_embeddings("test"))
        self.qdrant.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(size=size, distance=Distance.COSINE)
        )


    def add_point(self, collection_name: str, text: str, payload: dict[str, Any]) -> None:
        self.qdrant.upsert(
            collection_name=collection_name,
            points=[
                PointStruct(
                    id=str(uuid4().hex),
                    vector=self.get_embeddings(text),
                    payload=payload
                )
            ])


    @staticmethod
    def make_chunks(long_text: str) -> list[str]:
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=300,
            chunk_overlap=50,
            separators=["\n\n","\n", ". ", "! ", "? ", ]
        )
        chunks: list[str] = splitter.split_text(long_text)
        print(chunks)
        return chunks


    def clear_text_to_embedding(self, text: str) -> str:
        message: list[dict[str, Any]] = [{"role": "user", "text": f"""{chunking_prompt} 
                                           –æ–±—Ä–∞–±–æ—Ç–∞–π —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç: 
                                           {text}"""}]
        
        result: str = self.llm_model.get_sync_completion(messages=message, temperature=0.1)
        
        return result


    def read_json_and_add_point(self, doc_path: str, collection_name: str) -> None:
        with open(doc_path, 'r') as file:
            data = json.loads(file.read())
            for doc in data.get('documents'):
                if doc.get('text_entities'):
                    full_text: list[str] = []
                    for text in doc.get('text_entities'):
                        if text.get('type') == 'plain':
                            clear_text = text.get('text').replace('\n', ' ').strip().lower()
                            full_text.append(clear_text)


                    text_to_embedd = ' '.join(full_text)
                    text_to_embedd = self.clear_text_to_embedding(text_to_embedd)
                    text_chunks = self.make_chunks(text_to_embedd)
                
                    for chunk in text_chunks:
                        payload = {
                            "content": chunk,
                            "metadata": { 
                                "doc_name": doc.get('doc_name'),
                                "doc_chapter": doc.get('doc_chapter')
                            }
                        }
                        self.add_point(collection_name, chunk, payload)


    def rag_query(self, collection_name: str, user_question: str, max_context_tokens: int=1000) -> str:
        """
        –í—ã–ø–æ–ª–Ω–∏—Ç—å RAG –∑–∞–ø—Ä–æ—Å —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
        
        Args:
            collection_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Qdrant
            user_question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            max_context_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
            
        Returns:
            –û—Ç–≤–µ—Ç LLM —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        """
        # –ü–æ–∏—Å–∫ –≤ Qdrant
        search_results = self.qdrant.query_points(
            collection_name=collection_name,
            query=self.get_embeddings(user_question),
            limit=10,
            with_payload=True
        )
        
        context_parts: list[str] = []
        sources: list[str] = []
        total_tokens: int = 0
        
        for point in search_results.points:
            if point.score >= 0.65:
                content: str = point.payload.get('content', '')
                metadata: Any = point.payload.get('metadata', {})
                source_info = f"{metadata.get('doc_name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')} ({metadata.get('doc_chapter', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')})"
                
                context_parts.append(f"[–ß–∞–Ω–∫ {len(context_parts)+1}] {content}")
                sources.append(source_info)
                
                total_tokens += len(content.split())
                if total_tokens > max_context_tokens:
                    break
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
        related_docs_context = ""
        if self.enable_kg and self.knowledge_graph:
            related_docs_context = self._enhance_context_with_knowledge_graph(
                user_question, 
                sources
            )
        
        if not context_parts and not related_docs_context:
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, —ç—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞."
        
        vector_context = "\n".join(context_parts)
        sources_list = " | ".join(sources)

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
        kg_context_prompt = ""
        if related_docs_context:
            kg_context_prompt = f"""
<–ù–ê–ß–ê–õ–û –ö–û–ù–¢–ï–ö–°–¢–ê –ò–ó –ì–†–ê–§–ê –ó–ù–ê–ù–ò–ô>
{related_docs_context}
<–ö–û–ù–ï–¶ –ö–û–ù–¢–ï–ö–°–¢–ê –ò–ó –ì–†–ê–§–ê –ó–ù–ê–ù–ò–ô>

–ò–°–ü–û–õ–¨–ó–£–ô –ò–ù–§–û–†–ú–ê–¶–ò–Æ –ò–ó –ì–†–ê–§–ê –ó–ù–ê–ù–ò–ô –¥–ª—è –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ —Å–≤—è–∑–∞–Ω–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏.
"""

        messages = [
            {
                "role": "system", 
                "text": f"""{main_prompt} 
<–ù–ê–ß–ê–õ–û –í–ï–ö–¢–û–†–ù–û–ì–û –ö–û–ù–¢–ï–ö–°–¢–ê>
{vector_context}
<–ö–û–ù–ï–¶ –í–ï–ö–¢–û–†–ù–û–ì–û –ö–û–ù–¢–ï–ö–°–¢–ê>
{kg_context_prompt}

–í–ê–ñ–ù–û: –í –∫–æ–Ω—Ü–µ –æ—Ç–≤–µ—Ç–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–∫–∞–∂–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
–ò—Å—Ç–æ—á–Ω–∏–∫–∏: [—Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é]

–ü—Ä–∏–º–µ—Ä: –ò—Å—Ç–æ—á–Ω–∏–∫–∏: –¢—Ä—É–¥–æ–≤–æ–π –∫–æ–¥–µ–∫—Å –†–§ (–û—Ö—Ä–∞–Ω–∞ —Ç—Ä—É–¥–∞), –ü—Ä–∞–≤–∏–ª–∞ –ø–æ–∂–∞—Ä–Ω–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ (–û–±—â–∏–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è)"""
            },
            {
                "role": "user", 
                "text": f"{user_question}\n\n–ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞: {sources_list}"
            }
        ]
        
        answer: str = self.llm_model.get_sync_completion(messages=messages, temperature=0.1)
        return f"{answer}\n"
    
    def _enhance_context_with_knowledge_graph(self, user_question: str, current_sources: list[str]) -> str:
        """
        –†–∞—Å—à–∏—Ä–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
        
        Args:
            user_question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            current_sources: –¢–µ–∫—É—â–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑ Qdrant
            
        Returns:
            –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
        """
        try:
            enhanced_context = []
            
            # 1. –ü–æ–∏—Å–∫ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
            content_results = self.knowledge_graph.search_documents_by_content(user_question)
            if content_results:
                enhanced_context.append("üìÑ –°–≤—è–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ —Å–∏—Å—Ç–µ–º–µ:")
                for doc in content_results[:5]:  # –¢–æ–ø 5
                    enhanced_context.append(f"  - {doc['doc_id']}: {doc['doc_title']} "
                                          f"(—Å–µ–∫—Ü–∏—è: {doc['section_title']})")
            
            # 2. –ü–æ–∏—Å–∫ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤, –µ—Å–ª–∏ –æ–Ω–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã
            conflicts = self.knowledge_graph.find_conflicts()
            if conflicts:
                enhanced_context.append("\n‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏:")
                for conflict in conflicts[:3]:  # –¢–æ–ø 3
                    if '–∑–∞—Ç—è–∂–∫–∞' in user_question.lower() or '–±–æ–ª—Ç' in user_question.lower():
                        enhanced_context.append(f"  - {conflict['doc1_id']} ‚Üî {conflict['doc2_id']}: "
                                              f"{conflict['description']}")
            
            # 3. –ü–æ–∏—Å–∫ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö —Å—Å—ã–ª–æ–∫
            obsolete = self.knowledge_graph.find_obsolete_references()
            if obsolete:
                enhanced_context.append("\nüîÑ –£—Å—Ç–∞—Ä–µ–≤—à–∏–µ —Å—Å—ã–ª–∫–∏:")
                for ref in obsolete[:3]:  # –¢–æ–ø 3
                    enhanced_context.append(f"  - {ref['obsolete_ref']} ‚Üí {ref['current_std']}")
            
            return "\n".join(enhanced_context)
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π: {e}")
            return ""
    
    def get_related_documents(self, doc_id: str, max_depth: int = 2) -> list[dict]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å–≤—è–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
        
        Args:
            doc_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
            max_depth: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ —Å–≤—è–∑–µ–π
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        if not self.enable_kg or not self.knowledge_graph:
            return []
        
        try:
            return self.knowledge_graph.find_related_documents(doc_id, max_depth)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            return []
    
    def get_document_conflicts(self) -> list[dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"""
        if not self.enable_kg or not self.knowledge_graph:
            return []
        
        try:
            return self.knowledge_graph.find_conflicts()
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤: {e}")
            return []
    
    def find_documents_by_term(self, term: str) -> list[dict]:
        """–ù–∞–π—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã, —É–ø–æ–º–∏–Ω–∞—é—â–∏–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–π —Ç–µ—Ä–º–∏–Ω"""
        if not self.enable_kg or not self.knowledge_graph:
            return []
        
        try:
            return self.knowledge_graph.find_documents_by_term(term)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø–æ —Ç–µ—Ä–º–∏–Ω—É: {e}")
            return []
    
    def get_term_definition(self, term: str) -> Optional[dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–∞ –∏–∑ –≥–ª–æ—Å—Å–∞—Ä–∏—è"""
        if not self.enable_kg or not self.knowledge_graph:
            return None
        
        try:
            return self.knowledge_graph.find_term_definition(term)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è: {e}")
            return None
    
    def ensure_collection(self, collection_name: str) -> None:
        try:
            collections = self.qdrant.get_collections()
            if collection_name not in [c.name for c in collections.collections]:
                self.create_collection(collection_name)
        except Exception as e:
            print(f"An error occured: {e}")


    def load_documents(self, docs_path: str, collection_name: str) -> None:
        self.ensure_collection(collection_name)
        try:
            if os.path.isdir(docs_path):
                for doc in os.listdir(docs_path):
                    if doc.endswith(".json"):
                        self.read_json_and_add_point(os.path.join(docs_path, doc), collection_name)
            else:
                if docs_path.endswith(".json"):
                    self.read_json_and_add_point(docs_path, collection_name)
        except Exception as e:
            print(f"An error occured: {e}")


    def ask(self, user_question: str, collection_name: str) -> str:
        return self.rag_query(collection_name, user_question)
    
    def close(self) -> None:
        self.qdrant.close()
        if self.knowledge_graph:
            self.knowledge_graph.close()
